{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be455a30",
   "metadata": {},
   "source": [
    "### Validating the Extracted Expert-Referenced Vocal Cues using simple classifer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7105b40",
   "metadata": {},
   "source": [
    "Select Crema-D dataset and same instances that are selected for explanation. Apply the data preprocessing the same as the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92149725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f5306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and the preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53955b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187e285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e571c5ec",
   "metadata": {},
   "source": [
    "# Extract Expert-Referenced Vocal cues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d2cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top_human_ref(data, label):\n",
    "    \"\"\"\n",
    "    Extract vocal-cue intervals after preprocessing:\n",
    "      - trim to first non-silent region,\n",
    "      - convert to mono,\n",
    "      - resample to 16 kHz,\n",
    "      - fix length to 2.0 s,\n",
    "      - window = 30 ms, hop = 15 ms,\n",
    "    then return top/bottom intervals for loudness (RMS), spectral centroid,\n",
    "    and frame-wise jitter/shimmer (Parselmouth).\n",
    "    \"\"\"\n",
    "    # Load arbitrary audio file/bytes path with pydub\n",
    "    audio = AudioSegment.from_file(data)\n",
    "\n",
    "    # Detect non-silent regions (pydub works in milliseconds)\n",
    "    '''The term dB FS (or dBFS) means decibels relative to full scale. It is used for amplitude levels in digital systems with a maximum available peak level, e.g., PCM encoding, where 0 dB FS is assigned to the maximum level. A signal that reaches 50 percent of the maximum level would, for example, have a value of -6 dB FS. All peak measurements will be negative numbers.'''\n",
    "    silence_thresh = audio.dBFS - 14  # ~14 dB below average loudness\n",
    "    '''Recommended min_silence_len values (speech emotion datasets, 16 kHz audio):\n",
    "\n",
    "        200–300 ms → good default. Keeps micro-pauses and hesitation that are emotionally relevant, while trimming clear gaps.\n",
    "\n",
    "        ≤150 ms → too aggressive; may cut natural pauses in emotional speech.\n",
    "\n",
    "        ≥500 ms → safer for noisy data, but risks leaving long silences at beginning/end'''\n",
    "    nonsilent = detect_nonsilent(audio, min_silence_len=350, silence_thresh=silence_thresh)\n",
    "\n",
    "    # Keep the first non-silent interval if present\n",
    "    if nonsilent:\n",
    "        start_ms, end_ms = nonsilent[0][0], nonsilent[-1][1]\n",
    "        audio = audio[start_ms:end_ms]\n",
    "\n",
    "    # Ensure mono and target sampling rate\n",
    "    sr = 24000 \n",
    "    audio = audio.set_channels(1).set_frame_rate(sr)\n",
    "\n",
    "    # Convert to NumPy (int16 -> float32 in [-1, 1])\n",
    "    samples = np.array(audio.get_array_of_samples(), dtype=np.int16)\n",
    "    y = np.array(audio.get_array_of_samples(), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "    # Ensure a fixed length of 2 seconds (pad/truncate at the end)\n",
    "    target_len = int(sr * 2.0)\n",
    "    y = librosa.util.fix_length(y, size=target_len)\n",
    "\n",
    "    # Windowing params (kept as in your code) \n",
    "    '''From Data processing section: a window length of 480 samples, and a hop size of 240 samples\n",
    "    Then:\n",
    "      Window duration = 480 / 24000 = 0.030 seconds (20 ms)\n",
    "      Hop duration = 240 / 24000 = 0.015 seconds (10 ms)\n",
    "      Overlap = 480 - 240 = 240 samples (i.e., 50% overlap).\n",
    "    '''\n",
    "    window_size = int(sr * 0.10)   # 0.15 s window\n",
    "    hop_length = int(sr * 0.03)    # 0.25 s hop\n",
    "    overlap_length = window_size - hop_length\n",
    "    print(f\"Window Size: {window_size}, Hop Length: {hop_length}, Overlap Length: {overlap_length}\")\n",
    "\n",
    "    # Ensure y is at least window_size long\n",
    "    if len(y) < window_size:\n",
    "        raise ValueError(f\"Audio length {len(y)} is shorter than the window size {window_size}.\")\n",
    "    \n",
    "    '''TODO: Apply audio segmentation before feature extraction'''\n",
    "\n",
    "    #_____________________________________\n",
    "    # --- Features ---\n",
    "    loudness = librosa.feature.rms(\n",
    "        y=y, frame_length = window_size, hop_length=hop_length, center=False\n",
    "    ).flatten()\n",
    "    # Calculate spectral centroid (shrillness)\n",
    "    # Spectral centroid is a measure of the \"center of mass\" of the spectrum, often associated with shrillness\n",
    "    # It indicates how \"high\" the sound is perceived\n",
    "    # Higher values indicate a more shrill sound, while lower values indicate a deeper sound.\n",
    "    # It is calculated as the weighted mean of the frequencies present in the sound, weighted by their magnitudes.\n",
    "    # It is often used in audio analysis to characterize the timbre of a sound.\n",
    "    # librosa.feature.spectral_centroid returns the spectral centroid in Hz\n",
    "    # The result is a 2D array, so we flatten it to get a 1D array.\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(\n",
    "        y=y, sr=sr, n_fft=window_size,win_length=window_size, hop_length=hop_length, center=False\n",
    "    ).flatten()\n",
    "    #spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length).flatten()\n",
    "\n",
    "\n",
    "    # Parselmouth (Praat) jitter/shimmer on the preprocessed waveform\n",
    "    sound = parselmouth.Sound(y, sampling_frequency=sr)\n",
    "    point_process = call(sound, \"To PointProcess (periodic, cc)\", 90, 500)\n",
    "    jitter_local = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    shimmer_local = call([sound, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    # Approximate frame-wise jitter/shimmer\n",
    "    framewise_jitter, framewise_shimmer = [], []\n",
    "    for i in range(0, len(y) - window_size + 1, hop_length):\n",
    "        frame = y[i:i + window_size]\n",
    "        temp_sound = parselmouth.Sound(frame, sampling_frequency=sr)\n",
    "        temp_pp = call(temp_sound, \"To PointProcess (periodic, cc)\", 90, 500)\n",
    "        try:\n",
    "            fj = call(temp_pp, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "            fs = call([temp_sound, temp_pp], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "        except Exception:\n",
    "            fj, fs = 0.0, 0.0\n",
    "        framewise_jitter.append(fj)\n",
    "        framewise_shimmer.append(fs)\n",
    "\n",
    "    framewise_jitter = np.asarray(framewise_jitter)\n",
    "    framewise_shimmer = np.asarray(framewise_shimmer)\n",
    "\n",
    "    # Index -> (start, end) in seconds\n",
    "    def time_step(idx, hop_length, sr):\n",
    "        start = (idx * hop_length) / sr\n",
    "        end = ((idx * hop_length) + window_size) / sr\n",
    "        return (start, end)\n",
    "    \n",
    "    # returns the indices of the biggest k values, in decending order (higher -> lower).\n",
    "    def top_k_intervals(values, k=5):\n",
    "            values = np.asarray(values)\n",
    "            top_k_indices = np.argsort(values)[-k:][::-1]\n",
    "            return [time_step(idx, hop_length, sr) for idx in top_k_indices]\n",
    "    \n",
    "    # returns the indices of the smallest k values, in ascending order (lowest -> higher).\n",
    "    def bottom_k_intervals(values, k):\n",
    "            values = np.asarray(values)\n",
    "            idx = np.argsort(values)[:k]\n",
    "            return [time_step(i, hop_length, sr) for i in idx]\n",
    "    # High-arousal emotions\n",
    "    # angry, fear, happy\n",
    "    if label in {\"angry\", \"fear\", \"happy\"}:\n",
    "        top_loudness_intervals = top_k_intervals(loudness, k=5)\n",
    "        top_shrillness_intervals = top_k_intervals(spectral_centroid, k=5)\n",
    "        top_jitter_intervals = top_k_intervals(framewise_jitter, k=5)\n",
    "        top_shimmer_intervals = top_k_intervals(framewise_shimmer, k=5)\n",
    "\n",
    "        print(\"Top 5 Time Intervals of Loudness above avg value:\", top_loudness_intervals)\n",
    "        print(\"Top 5 Time Intervals of Shrillness above avg value:\", top_shrillness_intervals)\n",
    "        print(\"Top 5 Time Intervals of Jitter Intervals:\", top_jitter_intervals)\n",
    "        print(\"Top 5 Time Intervals of Shimmer Intervals:\", top_shimmer_intervals)\n",
    "\n",
    "        print(\"Total human-referenced segments for Loudness:\", len(loudness))\n",
    "        print(\"Total human-referenced segments for Shrillness:\", len(spectral_centroid))\n",
    "        print(\"Total human-referenced segments for Jitter:\", len(framewise_jitter))\n",
    "        print(\"Total human-referenced segments for Shimmer:\", len(framewise_shimmer))\n",
    "\n",
    "        return top_loudness_intervals, top_shrillness_intervals, top_jitter_intervals, top_shimmer_intervals\n",
    "    # Low-arousal emotions\n",
    "    # sad, neutral\n",
    "    elif label in {\"sad\", \"neutral\"}:\n",
    "        lowest_loudness = bottom_k_intervals(loudness, k=5)\n",
    "        lowest_shrillness = bottom_k_intervals(spectral_centroid, k=5)\n",
    "        lowest_jitter = bottom_k_intervals(framewise_jitter, k=5)\n",
    "        lowest_shimmer = bottom_k_intervals(framewise_shimmer, k=5)\n",
    "\n",
    "        print(\"lowest_loudness:\", lowest_loudness)\n",
    "        print(\"lowest_shrillness:\", lowest_shrillness)\n",
    "        print(\"Lowest Jitter:\", lowest_jitter)\n",
    "        print(\"Lowest Shimmer:\", lowest_shimmer)\n",
    "\n",
    "        return lowest_loudness, lowest_shrillness, lowest_jitter, lowest_shimmer\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported label. Use 'angry', 'fear', 'happy', 'sad', or 'neutral'.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
